{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GonzaloMA-17/RL_FJLGM/blob/main/notebook_aproximaciones_4x4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzTOlsjjGWvz"
      },
      "source": [
        "**Asignatura**: Extensiones de Machine Learning, 2024/2025\n",
        "\n",
        "**Alumnos**:<br>\n",
        "- Gonzalo Marcos Andr√©s (gonzalo.marcosa@um.es)\n",
        "- Francisco Jos√© L√≥pez Fern√°ndez (franciscojose.lopezf@um.es)\n",
        "\n",
        "**M√°ster de Inteligencia Artificial**\n",
        "\n",
        "| **Facultad de Inform√°tica** | **Universidad de Murcia** |\n",
        "|-----------------------------|---------------------------|\n",
        "| ![](https://www.um.es/image/layout_set_logo?img_id=175281&t=1726728636242) | ![](https://www.um.es/o/um-lr-principal-um-home-theme/images/logo-um.png) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KMAdaGOGWv1"
      },
      "source": [
        "# **Pr√°ctica 2.  Aprendizaje en entornos complejos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXLaX6uZGWv2"
      },
      "source": [
        "## **1. Preparaci√≥n del Entorno**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GervG_N0GWv3"
      },
      "source": [
        "### 1.1 Introducci√≥n.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZtQiiYzGWv3"
      },
      "source": [
        "Este notebook se enfoca en la implementaci√≥n y an√°lisis de un agente que utiliza **Aprendizaje por Refuerzo** para resolver distintos entornos que nos ofrece `gymnasium`.  \n",
        "\n",
        "Los entornos que vamos a utlizar son:\n",
        "\n",
        " - **FrozenLake (4x4)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFpQlX2oGWv4"
      },
      "source": [
        "### 1.2. Instalaci√≥n de Dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "95c1jiWIGWv4"
      },
      "source": [
        "Se instalan las librer√≠as necesarias para trabajar con el entorno de simulaci√≥n `gymnasium`, lo que permite crear un ambiente controlado donde el agente pueda interactuar y aprender. Este entorno simula una cuadr√≠cula donde el agente debe navegar para alcanzar una meta, mientras se enfrenta a varios desaf√≠os."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "REXKqijvGWv5"
      },
      "outputs": [],
      "source": [
        "!  git clone https://github.com/GonzaloMA-17/RL_FJLGM.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd RL_FJLGM"
      ],
      "metadata": {
        "id": "k4wjwRWGGu3N",
        "outputId": "945be1c6-701d-4b37-abfa-595fe9f6aa38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'RL_FJLGM'\n",
            "/content/RL_FJLGM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p_4uevDpGWv6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# ! pip install 'gym[box2d]==0.20.0'\n",
        "# ! pip install gymnasium[box2d]\n",
        "# ! pip install \"gymnasium[toy-text]\n",
        "# ! pip install gymnasium\n",
        "# ! pip install numpy\n",
        "# ! pip install matplotlib\n",
        "# ! pip install tqdm\n",
        "# ! pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lX7xNirGWv7"
      },
      "source": [
        "### 1.3 Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wcORu3ukGWv7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import gc\n",
        "import torch\n",
        "from src_agents import *\n",
        "from src_plotting import *\n",
        "\n",
        "from src_agents.deepQLearning import *\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from src_agents.sarsaSemiGradiente import FrozenLakeWrapper, train_sarsa#, GraphVisualizer\n",
        "from src_plotting import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JDUQIXqGWv7"
      },
      "source": [
        "### 1.4 Importaci√≥n de los Entornos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qOptRNPGWv8"
      },
      "source": [
        "Se cargan dos entornos diferentes para el agente:\n",
        "- **FrozenLake (4x4)** es un peque√±o lago congelado en el que el agente debe navegar para llegar a su destino, evitando caer en el agua. Este entorno es especialmente √∫til para un aprendizaje inicial, ya que no es resbaladizo, lo que facilita la comprensi√≥n de c√≥mo el agente interact√∫a con el entorno.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbWZNW55GWv8"
      },
      "source": [
        "\n",
        "### 1.5 Funciones para Mostrar los Resultados\n",
        "Se definen varias funciones para graficar y visualizar los resultados del entrenamiento del agente. Estas funciones permiten analizar el desempe√±o del agente en diferentes aspectos.\n",
        "1.`plot(list_stats)`  \n",
        "   - **Descripci√≥n**: Graficar√° la proporci√≥n de recompensas obtenidas en cada episodio del entrenamiento. Este gr√°fico proporciona una visualizaci√≥n clara del progreso del agente, mostrando c√≥mo mejora su desempe√±o a medida que avanza en el proceso de aprendizaje.\n",
        "\n",
        "2. `plot_episode_lengths(episode_lengths, window=50)`  \n",
        "   - **Descripci√≥n**: Graficar√° la longitud de los episodios en el entrenamiento, y calcular√° y mostrar√° la tendencia de las longitudes utilizando una media m√≥vil.\n",
        "\n",
        "3. `plot_policy_blank(Q, env)`  \n",
        "   - **Descripci√≥n**: Dibuja la pol√≠tica aprendida por el agente sobre un entorno vac√≠o, representando las acciones √≥ptimas en cada estado de la **Q-table** del agente con flechas (izquierda, abajo, derecha, arriba).\n",
        "\n",
        "4. `plot_comparison(stats_list, labels, title=\"Comparaci√≥n de Resultados de Entrenamiento\")`  \n",
        "   - **Descripci√≥n**: Compara la evoluci√≥n de las recompensas obtenidas por diferentes algoritmos o agentes a lo largo del entrenamiento, permitiendo visualizar el rendimiento relativo de cada uno.\n",
        "\n",
        "5. `plot_episode_lengths_comparison(episode_lengths_list, labels, window=50, title=\"Comparaci√≥n de Longitudes de Episodios\")`  \n",
        "   - **Descripci√≥n**: Compara las longitudes de los episodios de diferentes agentes, mostrando las longitudes y las tendencias de cada uno en subgr√°ficas separadas, con sombreado y media m√≥vil.\n",
        "\n",
        "Todas las funciones y clases relacionadas con la visualizaci√≥n de gr√°ficos se encuentran en el archivo `src_agents/plotting.py`. Para utilizarlas, ser√° necesario importar las librer√≠as correspondientes desde esa ruta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Znjnc6QGWv8"
      },
      "source": [
        "## **2. Dise√±o del Agente**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_-bwY26GWv8"
      },
      "source": [
        "### 2.1 Estructura del Agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtbip1w6GWv8"
      },
      "source": [
        "Este es la estructura que contiene la implementaci√≥n de distintos agentes, organizados en m√≥dulos espec√≠ficos para facilitar su uso y mantenimiento.  \n",
        "\n",
        "Cada agente hereda de una clase base (`agent.py`) y utiliza diferentes estrategias de aprendizaje, como **Monte Carlo**, **Q-Learning** y **SARSA**. Adem√°s, se incluyen m√≥dulos auxiliares para definir pol√≠ticas de exploraci√≥n y visualizar resultados.\n",
        "\n",
        "```plaintext\n",
        "|-- üìÇ src_agents                                    # Carpeta principal que contiene los agentes de Aprendizaje por Refuerzo\n",
        "|   |-- üìÑ __init__.py                               # Archivo que convierte el directorio en un paquete de Python\n",
        "|   |-- üìÑ agent.py                                  # Clase base para todos los agentes\n",
        "|   |-- üìÑ deepQLearning.py                          # Implementaci√≥n del agente Deep Q-Learning (DQN)\n",
        "|   |-- üìÑ monteCarloOnPolicy.py                     # Implementaci√≥n del agente Monte Carlo On-Policy\n",
        "|   |-- üìÑ monteCarloOffPolicy.py                    # Implementaci√≥n del agente Monte Carlo Off-Policy\n",
        "|   |-- üìÑ qLearning.py                              # Implementaci√≥n del agente Q-Learning\n",
        "|   |-- üìÑ sarsa.py                                  # Implementaci√≥n del agente SARSA tabular\n",
        "|   |-- üìÑ sarsaSemiGradiente.py                     # Implementaci√≥n del agente SARSA Semigradiente\n",
        "|   |-- üìÑ politicas.py                              # Definici√≥n de pol√≠ticas de exploraci√≥n como epsilon-greedy y softmax\n",
        "\n",
        "|-- üìÇ src_plotting                                  # Carpeta con herramientas de visualizaci√≥n de resultados\n",
        "|   |-- üìÑ __init__.py                               # Archivo que convierte el directorio en un paquete de Python\n",
        "|   |-- üìÑ plotting.py                               # Funciones de visualizaci√≥n de datos y gr√°ficos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXLfkgB6GWv9"
      },
      "source": [
        "El dise√±o del agente consta de dos partes.  \n",
        "\n",
        "- Las pol√≠ticas (toma de decisiones) que realiza.\n",
        "- El algoritmo con el que aprende."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x0Wqu52GWv9"
      },
      "source": [
        "### 2.2 Pol√≠ticas del Agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R8GbIKwGWv9"
      },
      "source": [
        "- **Epsilon-soft**: Se define una pol√≠tica donde todas las acciones tienen una probabilidad de ser elegida.\n",
        "   \n",
        "- **Pol√≠tica epsilon-greedy**: basada en la pol√≠tica epsilon-soft. De esta forma el agente tiene una peque√±a probabilidad de explorar (tomar una acci√≥n aleatoria) y una mayor probabilidad de explotar (tomar la acci√≥n que considera mejor). Esto permite equilibrar la exploraci√≥n y la explotaci√≥n.  \n",
        "\n",
        "- **Pol√≠tica greedy**: Es la usada una vez que \"ha aprendido\".  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nztilHa-GWv9"
      },
      "source": [
        "### 2.3 Algoritmos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkHbYqxPGWv9"
      },
      "source": [
        "\n",
        "- **Tabulares**  \n",
        "  - Monte Carlo On Policy  \n",
        "  - Monte Carlo Off Policy  \n",
        "  - SARSA\n",
        "  - Q - Learning\n",
        "\n",
        "- **Control con Aproximaciones**\n",
        "  - SARSA semigradiente\n",
        "  - Deep Q - Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jpVkuEhGWv9"
      },
      "source": [
        "## **3. Experimentaci√≥n - Frozen Lake**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qdmJWoRGWv9"
      },
      "source": [
        "FrozenLake es un entorno de Aprendizaje por Refuerzo incluido en Gymnasium, en el que un agente debe aprender a desplazarse sobre una superficie helada para llegar a un objetivo sin caer en agujeros. Se representa como una cuadr√≠cula donde cada celda puede ser suelo firme, un agujero o la meta.\n",
        "\n",
        "El agente puede moverse en cuatro direcciones: izquierda, derecha, arriba y abajo. En la versi√≥n est√°ndar del entorno, el hielo introduce un factor de aleatoriedad en los movimientos, lo que significa que el agente no siempre se desplaza en la direcci√≥n elegida. Sin embargo, en este caso **configuraremos el entorno sin deslizamiento**, lo que significa que el agente se mover√° exactamente en la direcci√≥n que elija sin desviaciones aleatorias. Esto hace que el problema sea m√°s determinista y permite un aprendizaje m√°s directo de las estrategias √≥ptimas.\n",
        "\n",
        "El objetivo del agente es llegar a la casilla de meta desde la posici√≥n inicial sin caer en un agujero. Se utiliza un sistema de recompensas en el que el agente recibe una recompensa de 1 cuando alcanza la meta y 0 en cualquier otro caso. Al eliminar el deslizamiento, el agente puede aprender una pol√≠tica √≥ptima de manera m√°s eficiente, ya que sus acciones tienen consecuencias predecibles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b5BO0kGGWv-"
      },
      "source": [
        "### 3.5 Deep Q - Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTA165lJGWv-"
      },
      "source": [
        "# Deep Q-Learning (DQN)\n",
        "\n",
        "Deep Q-Learning (DQN) es una extensi√≥n de Q-Learning que utiliza redes neuronales para aproximar la funci√≥n de valor de estado-acci√≥n $ Q(s, a) $, permitiendo manejar espacios de estado continuos o de alta dimensi√≥n donde el enfoque tabular no es viable.\n",
        "\n",
        "\n",
        "DQN utiliza una red neuronal que recibe el estado $ s $ como entrada y predice los valores $ Q(s, a) $ para cada acci√≥n posible. La actualizaci√≥n de $ Q(s, a) $ sigue la ecuaci√≥n:\n",
        "\n",
        "$y = r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a')$\n",
        "\n",
        "donde $ Q_{\\text{target}} $ es una copia de la red principal que se actualiza peri√≥dicamente para estabilizar el entrenamiento.\n",
        "\n",
        "\n",
        "- **Replay Buffer**: Se almacenan transiciones $ (s, a, r, s') $ para romper la correlaci√≥n entre experiencias y mejorar la estabilidad del aprendizaje.  \n",
        "- **Target Network**: Se mantiene una red objetivo congelada que se actualiza cada cierto n√∫mero de pasos para evitar oscilaciones en el entrenamiento.  \n",
        "- **Exploraci√≥n con $ \\epsilon $-greedy**: Se usa $ \\epsilon $-greedy con un decaimiento progresivo de $ \\epsilon $ para favorecer la exploraci√≥n al inicio y la explotaci√≥n en etapas avanzadas.  \n",
        "- **Funci√≥n de p√©rdida**: Se minimiza el error cuadr√°tico medio (MSE) entre la predicci√≥n de la red y la estimaci√≥n corregida:\n",
        "\n",
        "$L(\\theta) = \\mathbb{E} \\left[ \\left( y - Q(s, a; \\theta) \\right)^2 \\right]$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4tm_vsUGWv-"
      },
      "source": [
        "#### Mapa 4x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bapmIivbGWv-"
      },
      "source": [
        "Este fragmento de c√≥digo establece una semilla fija para la generaci√≥n de n√∫meros aleatorios en varias bibliotecas, lo que garantiza la **reproducibilidad** en experimentos de Machine Learning o Aprendizaje por Refuerzo.\n",
        "\n",
        "- `seed = 1995`: Se define una semilla fija con el valor `1995`.\n",
        "\n",
        "- `random.seed(seed)`: Establece la semilla para la librer√≠a `random` de Python.\n",
        "\n",
        "- `np.random.seed(seed)`: Fija la semilla para `NumPy`, garantizando que las funciones aleatorias de numpy generen los mismos valores en ejecuciones repetidas.\n",
        "\n",
        "- `torch.manual_seed(seed)`: Configura la semilla en PyTorch para que las operaciones aleatorias en tensores de CPU sean reproducibles.\n",
        "\n",
        "- `torch.cuda.manual_seed(seed)`: Si hay una GPU, fija la semilla en CUDA para asegurar la reproducibilidad en c√°lculos realizados en la GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PcNvJfkCGWv-"
      },
      "outputs": [],
      "source": [
        "seed = 1995\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s0qtTmWGWv-"
      },
      "source": [
        "El siguiente c√≥digo crea un entorno de **FrozenLake** en Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2Z5AiFVAGWv-"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', is_slippery=False, map_name=\"4x4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDVs_ruJGWv-"
      },
      "source": [
        "Realizamos el entrenamiento de un agente en el entorno **FrozenLake** utilizando el algoritmo **Deep Q-Learning (DQN)**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJh1x5CAGWv_",
        "outputId": "4c43f67d-5782-4536-c1ab-241d6ed73d14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50, Average Reward: 0.00, Epsilon: 0.78\n",
            "Episode 100, Average Reward: 0.02, Epsilon: 0.61\n",
            "Episode 150, Average Reward: 0.26, Epsilon: 0.47\n",
            "Episode 200, Average Reward: 0.62, Epsilon: 0.37\n",
            "Episode 250, Average Reward: 0.72, Epsilon: 0.29\n",
            "Episode 300, Average Reward: 0.68, Epsilon: 0.22\n",
            "Episode 350, Average Reward: 0.76, Epsilon: 0.17\n",
            "Episode 400, Average Reward: 0.82, Epsilon: 0.13\n",
            "Episode 450, Average Reward: 0.84, Epsilon: 0.10\n",
            "Episode 500, Average Reward: 0.88, Epsilon: 0.08\n",
            "Episode 550, Average Reward: 0.92, Epsilon: 0.06\n",
            "Episode 600, Average Reward: 0.90, Epsilon: 0.05\n",
            "Episode 650, Average Reward: 1.00, Epsilon: 0.04\n",
            "Episode 700, Average Reward: 0.98, Epsilon: 0.03\n",
            "Episode 750, Average Reward: 1.00, Epsilon: 0.02\n",
            "Episode 800, Average Reward: 1.00, Epsilon: 0.02\n",
            "Episode 850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 1050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1250, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 1300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 1350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 1500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1550, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 1600, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 1650, Average Reward: 0.62, Epsilon: 0.01\n",
            "Episode 1700, Average Reward: 0.88, Epsilon: 0.01\n",
            "Episode 1750, Average Reward: 0.72, Epsilon: 0.01\n",
            "Episode 1800, Average Reward: 0.08, Epsilon: 0.01\n",
            "Episode 1850, Average Reward: 0.38, Epsilon: 0.01\n",
            "Episode 1900, Average Reward: 0.54, Epsilon: 0.01\n",
            "Episode 1950, Average Reward: 0.50, Epsilon: 0.01\n",
            "Episode 2000, Average Reward: 0.28, Epsilon: 0.01\n",
            "Episode 2050, Average Reward: 0.74, Epsilon: 0.01\n",
            "Episode 2100, Average Reward: 0.60, Epsilon: 0.01\n",
            "Episode 2150, Average Reward: 0.34, Epsilon: 0.01\n",
            "Episode 2200, Average Reward: 0.26, Epsilon: 0.01\n",
            "Episode 2250, Average Reward: 0.62, Epsilon: 0.01\n",
            "Episode 2300, Average Reward: 0.76, Epsilon: 0.01\n",
            "Episode 2350, Average Reward: 0.88, Epsilon: 0.01\n",
            "Episode 2400, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 2450, Average Reward: 0.62, Epsilon: 0.01\n",
            "Episode 2500, Average Reward: 0.44, Epsilon: 0.01\n",
            "Episode 2550, Average Reward: 0.62, Epsilon: 0.01\n",
            "Episode 2600, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 2650, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 2700, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 2750, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 2800, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 2850, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 2900, Average Reward: 0.70, Epsilon: 0.01\n",
            "Episode 2950, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 3000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 3050, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 3100, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 3150, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 3200, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 3250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 3300, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 3350, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 3400, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 3450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 3500, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 3550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 3600, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 3650, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 3700, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 3750, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 3800, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 3850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 3900, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 3950, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 4000, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 4050, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4100, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 4150, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 4200, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 4250, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 4300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 4350, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 4400, Average Reward: 0.82, Epsilon: 0.01\n",
            "Episode 4450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 4500, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 4550, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4600, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 4650, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4700, Average Reward: 0.88, Epsilon: 0.01\n",
            "Episode 4750, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4800, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4850, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 4900, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4950, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 5000, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 5050, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 5100, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 5150, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 5200, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 5250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 5300, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 5350, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 5400, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 5450, Average Reward: 0.78, Epsilon: 0.01\n",
            "Episode 5500, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 5550, Average Reward: 0.82, Epsilon: 0.01\n",
            "Episode 5600, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 5650, Average Reward: 0.70, Epsilon: 0.01\n",
            "Episode 5700, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 5750, Average Reward: 0.86, Epsilon: 0.01\n",
            "Episode 5800, Average Reward: 0.86, Epsilon: 0.01\n",
            "Episode 5850, Average Reward: 0.66, Epsilon: 0.01\n",
            "Episode 5900, Average Reward: 0.76, Epsilon: 0.01\n",
            "Episode 5950, Average Reward: 0.78, Epsilon: 0.01\n",
            "Episode 6000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6050, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 6100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 6300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6350, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 6400, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 6450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 6500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6550, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 6600, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 6650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 6750, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 6800, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 6850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6950, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 7000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7050, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7150, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7250, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 7300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7400, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7550, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7650, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7800, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7950, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 8000, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 8050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 8500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 8550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8650, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 8700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9100, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9150, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 9200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9350, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 9400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9550, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9750, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9850, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9900, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9950, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10150, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10750, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10850, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 10900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11150, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 11200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 11300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11450, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 11500, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 11550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11650, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 11700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 11750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11800, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 11850, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 11900, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 11950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12100, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 12150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 12300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12400, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 12450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12650, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 12700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12850, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 12900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13650, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13900, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13950, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 14000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 14050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 14250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 14550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14900, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 14950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15050, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 15100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15150, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 15200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15300, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 15350, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 15400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 15550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16050, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 16100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16350, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 16400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16650, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16800, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16950, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17050, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 17100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17400, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17600, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17750, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17950, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18050, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18100, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18650, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 18700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18800, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 18850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18900, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 18950, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 19000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19350, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19600, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20550, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20600, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20750, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20850, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20900, Average Reward: 0.80, Epsilon: 0.01\n",
            "Episode 20950, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 21000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 21050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 21100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 21150, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 21200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 21250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 21300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 21350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 21400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 21450, Average Reward: 1.00, Epsilon: 0.01\n"
          ]
        }
      ],
      "source": [
        "episode_rewards_DQL, episode_lengths_DQL, training_errors_DQL, policy_net, target_net_DQL = train_dqn(env, num_episodes=25000, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw-IWMX4GWv_"
      },
      "source": [
        "**Conclusi√≥n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QrvtFZ8GWv_"
      },
      "outputs": [],
      "source": [
        "plot_all(episode_rewards_DQL, episode_lengths_DQL, training_errors_DQL, rolling_length=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPYTYBmNGWv_"
      },
      "source": [
        "Analizando las gr√°ficas obtenidas durante el entrenamiento del agente con DQN en el entorno FrozenLake 4x4, podemos destacar los siguientes puntos:\n",
        "\n",
        "- **Evoluci√≥n de las recompensas** (Gr√°fica izquierda)\n",
        "Al comienzo del entrenamiento, la recompensa media es baja y presenta fluctuaciones considerables, reflejando la fase de exploraci√≥n inicial del agente.\n",
        "A medida que avanzan los episodios (antes de llegar a los 2,000 aproximadamente), la recompensa aumenta de manera notable hasta estabilizarse cerca de valores pr√≥ximos a 1. Esto indica que el agente ha aprendido a maximizar su probabilidad de alcanzar la meta.\n",
        "En la etapa final, la recompensa se mantiene estable en valores altos, lo cual sugiere que el agente ha convergido a una pol√≠tica casi √≥ptima, pese a la naturaleza estoc√°stica del entorno.\n",
        "\n",
        "- **Evoluci√≥n de la longitud de los episodios** (Gr√°fica derecha)\n",
        "Durante los primeros episodios, se observan picos de duraci√≥n muy elevados (llegando a superar 80 pasos), se√±al de que el agente todav√≠a no cuenta con una estrategia clara y est√° probando m√∫ltiples rutas sin √©xito.\n",
        "Con el progreso del entrenamiento, la longitud de los episodios desciende dr√°sticamente, llegando a estabilizarse en valores bajos (cercanos a 5‚Äì10 pasos), lo que indica que el agente ha encontrado rutas eficientes para resolver el entorno.\n",
        "Hacia la fase final, la longitud se mantiene relativamente constante, reforzando la idea de que el agente aplica una pol√≠tica s√≥lida para llegar a la meta con el menor n√∫mero de movimientos posibles.\n",
        "\n",
        "**Conclusi√≥n general**\n",
        "El entrenamiento con Deep Q-Learning en FrozenLake 4x4 demuestra ser exitoso. El agente pasa de episodios largos con recompensas bajas a episodios m√°s cortos y recompensas cercanas al m√°ximo. La estabilidad observada en ambos gr√°ficos hacia el final del entrenamiento indica que el agente ha aprendido una pol√≠tica efectiva y robusta para navegar en un entorno estoc√°stico, maximizando su recompensa y reduciendo los movimientos innecesarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNNLjJopGWv_"
      },
      "source": [
        "### 3.6 Sarsa SemiGradiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7ZYPYcKGWv_"
      },
      "source": [
        "SARSA semigradiente es una extensi√≥n del algoritmo SARSA tradicional, dise√±ada para manejar entornos con espacios de estado continuos o de alta dimensi√≥n. En estos casos, la representaci√≥n tabular de $ Q(s, a) $ se vuelve inviable, por lo que se emplean funciones de aproximaci√≥n en lugar de almacenar valores discretos para cada par estado-acci√≥n.\n",
        "\n",
        "El m√©todo SARSA semigradiente utiliza una funci√≥n de aproximaci√≥n de la forma:\n",
        "\n",
        "$$\n",
        "Q(s, a; \\theta) \\approx f(s, a; \\theta)\n",
        "$$\n",
        "\n",
        "donde $ \\theta $ representa un conjunto de par√°metros ajustables que determinan la estimaci√≥n de los valores $ Q(s, a) $. En lugar de actualizar una tabla de valores $ Q $, se actualizan los par√°metros $ \\theta $ mediante descenso de gradiente.\n",
        "\n",
        "La actualizaci√≥n de los par√°metros $ \\theta $ sigue la regla de aprendizaje:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_{\\theta} Q(s, a; \\theta)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $\\alpha $ es la tasa de aprendizaje.\n",
        "- $ \\delta $ es el error temporal definido como:\n",
        "\n",
        "$$\n",
        "\\delta = R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}; \\theta) - Q(s, a; \\theta)\n",
        "$$\n",
        "\n",
        "- $ \\nabla_{\\theta} Q(s, a; \\theta) $ es el gradiente de la funci√≥n de aproximaci√≥n respecto a los par√°metros $ \\theta $.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFx8126dGWwA"
      },
      "source": [
        "#### Mapa 4x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cgomsbIGWwF"
      },
      "source": [
        "Comenzamos el c√≥digo estableciendo la semilla para los generadores de n√∫meros aleatorios en varias bibliotecas, asegurando que los experimentos sean reproducibles.\n",
        "- **`seed = 1995`**: Se define el valor de la semilla como `1995`.\n",
        "- **`random.seed(seed)`**: Establece la semilla en el m√≥dulo `random` de Python.\n",
        "-\n",
        "- **`np.random.seed(seed)`**: Establece la semilla en el generador de n√∫meros aleatorios de NumPy.\n",
        "-\n",
        "- **`torch.manual_seed(seed)`**: Establece la semilla para el generador de n√∫meros aleatorios de PyTorch en la CPU.\n",
        "-\n",
        "- **`if torch.cuda.is_available(): torch.cuda.manual_seed(seed)`**: Si se dispone de una GPU (es decir, si CUDA est√° disponible), esta l√≠nea establece la semilla para el generador de n√∫meros aleatorios de PyTorch en la GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FngIQNb0GWwF"
      },
      "outputs": [],
      "source": [
        "seed = 1995\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ul_hALYGWwG"
      },
      "source": [
        "En este fragmento de c√≥digo, se est√° creando un entorno utilizando la clase `FrozenLakeWrapper`. con el parametro ``is_slippery = False``, para crear un entorno que no sea resbaladizo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JsSvKHGGWwG"
      },
      "outputs": [],
      "source": [
        "# Usamos FrozenLakeWrapper para crear el entorno\n",
        "env_wrapper = FrozenLakeWrapper(is_slippery=False, map_name=\"4x4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FPkOqzcGWwG"
      },
      "source": [
        "Se entrena el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHZ7RSSWGWwG"
      },
      "outputs": [],
      "source": [
        "# Entrenar usando SARSA semigradiente (en vez de DQN)\n",
        "episode_rewards_SSG, episode_lengths_SSG, training_errors_SSG, agent = train_sarsa(env_wrapper, num_episodes=25000, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwRIjKR7GWwH"
      },
      "source": [
        "**Conclusi√≥n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3copHcJBGWwH"
      },
      "outputs": [],
      "source": [
        "plot_all(episode_rewards_SSG, episode_lengths_SSG, training_errors_SSG, rolling_length=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUAgP09rGWwH"
      },
      "source": [
        "Analizando las gr√°ficas obtenidas durante el entrenamiento del agente con **Sarsa semigradiente** en el entorno **FrozenLake**, podemos destacar los siguientes puntos:\n",
        "\n",
        "\n",
        "**Evoluci√≥n de las recompensas** (Gr√°fica izquierda)\n",
        "    - Al inicio del entrenamiento, las recompensas son bajas y muestran una alta variabilidad, lo cual indica que el agente se encuentra en fase de exploraci√≥n y a√∫n no ha desarrollado una estrategia clara.  \n",
        "    - A medida que avanza el entrenamiento (alrededor de los primeros miles de episodios), la recompensa media asciende r√°pidamente, situ√°ndose cerca de valores de 0.8‚Äì0.9. Esto sugiere que el agente aprende a tomar decisiones que le permiten alcanzar la meta con mayor frecuencia.  \n",
        "    - En la fase final, la recompensa se mantiene relativamente estable cerca de valores altos, lo que indica que el agente ha convergido a una pol√≠tica casi √≥ptima.\n",
        "\n",
        "\n",
        "**Evoluci√≥n de la longitud de los episodios** (Gr√°fica derecha)\n",
        "    - Durante los primeros episodios, la longitud de los mismos es relativamente alta y presenta fluctuaciones considerables, reflejando la exploraci√≥n intensa del agente y la ausencia de una ruta clara hacia la meta.  \n",
        "    - Conforme el agente adquiere experiencia, se observa una tendencia a la baja en la duraci√≥n de los episodios. Esto indica que el agente aprende a llegar a la meta en menos pasos o, en caso de fallar, lo hace antes, reduciendo el tiempo total del episodio.  \n",
        "    - En la etapa final del entrenamiento, la longitud de los episodios se estabiliza en valores m√°s bajos, lo que demuestra que el agente ha aprendido la ruta m√°s eficiente para resolver el entorno con el m√≠nimo n√∫mero de movimientos posibles.\n",
        "\n",
        "**Conclusi√≥n general**\n",
        "    El entrenamiento con **Sarsa semigradiente** en **FrozenLake** muestra resultados exitosos. El agente pasa de una fase de exploraci√≥n err√°tica, con bajas recompensas y episodios largos, a una estrategia consistente que maximiza la recompensa y minimiza la duraci√≥n de los episodios. Aunque el entorno estoc√°stico impide alcanzar un √©xito absoluto, el rendimiento final del agente es alto y refleja la convergencia hacia una pol√≠tica efectiva y estable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvk52b0YGWwH"
      },
      "source": [
        "## **4. Conclusi√≥n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LrqqtwpGWwI"
      },
      "outputs": [],
      "source": [
        "plot_comparative_results(episode_rewards_SSG, episode_lengths_SSG,\n",
        "                         episode_rewards_DQL, episode_lengths_DQL,\n",
        "                         label1=\"SARSA Semigradiente\", label2=\"DQN\",\n",
        "                         rolling_length=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7bPtnNHGWwI"
      },
      "source": [
        "Analizando las gr√°ficas comparativas de Sarsa Semigradiente (l√≠nea azul) y DQN (l√≠nea roja punteada) en el entorno FrozenLake 4x4, se pueden destacar los siguientes puntos:\n",
        "\n",
        "**Comparaci√≥n de Recompensas** (Gr√°fica izquierda)\n",
        "- Fase inicial:\n",
        "Tanto Sarsa Semigradiente como DQN comienzan con recompensas bajas e inestables, reflejando la etapa de exploraci√≥n y la falta de una estrategia definida.  \n",
        "\n",
        "- Progreso:\n",
        "Sarsa Semigradiente muestra un ascenso paulatino hasta situarse cerca de valores de recompensa cercanos a 1, manteni√©ndose relativamente estable en ese rango.\n",
        "DQN experimenta un incremento m√°s r√°pido en las recompensas, pero presenta picos de inestabilidad en algunos tramos, donde la recompensa cae bruscamente antes de recuperarse.\n",
        "\n",
        "Ambos m√©todos alcanzan recompensas promedio altas, lo que indica que aprenden pol√≠ticas eficaces para superar el entorno. Sin embargo, DQN exhibe mayor variabilidad.\n",
        "\n",
        "**Comparaci√≥n de Longitudes de Episodios** (Gr√°fica derecha)\n",
        "\n",
        "- Fase inicial:\n",
        "Al inicio, la duraci√≥n de los episodios para DQN es muy elevada, mientras que Sarsa Semigradiente se mantiene en rangos algo m√°s moderados, aunque tambi√©n inestables.  \n",
        "\n",
        "- Progreso:\n",
        "Con el paso de los episodios, ambas estrategias reducen significativamente la longitud media de los episodios. Esto sugiere que, al aprender la din√°mica del entorno, el agente encuentra rutas m√°s eficientes para llegar a la meta o, en caso de fallar, lo hace de forma m√°s r√°pida.\n",
        "\n",
        "Sarsa Semigradiente logra estabilizarse en valores relativamente bajos, mostrando una curva m√°s homog√©nea.\n",
        "DQN reduce tambi√©n la duraci√≥n de los episodios a valores bajos, pero sufre picos ocasionales que indican cierta inestabilidad en la pol√≠tica aprendida.\n",
        "\n",
        "**Conclusi√≥n general**\n",
        "Tanto Sarsa Semigradiente como DQN consiguen resolver de manera efectiva el entorno FrozenLake 4x4, alcanzando recompensas altas y reduciendo dr√°sticamente la longitud de los episodios. No obstante, se aprecian diferencias en la estabilidad del aprendizaje:\n",
        "\n",
        "Basandonos √∫nicamente en la estabilidad y la consistencia de los resultados, Sarsa Semigradiente parece ser la mejor opci√≥n en este experimento con FrozenLake 4x4. Aunque DQN puede alcanzar recompensas similares o incluso aprender un poco m√°s r√°pido en ciertos tramos, presenta m√°s fluctuaciones tanto en la recompensa como en la longitud de los episodios, lo que indica una pol√≠tica menos estable.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}