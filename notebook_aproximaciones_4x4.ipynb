{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GonzaloMA-17/RL_FJLGM/blob/main/notebook_aproximaciones_4x4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzTOlsjjGWvz"
      },
      "source": [
        "**Asignatura**: Extensiones de Machine Learning, 2024/2025\n",
        "\n",
        "**Alumnos**:<br>\n",
        "- Gonzalo Marcos Andrés (gonzalo.marcosa@um.es)\n",
        "- Francisco José López Fernández (franciscojose.lopezf@um.es)\n",
        "\n",
        "**Máster de Inteligencia Artificial**\n",
        "\n",
        "| **Facultad de Informática** | **Universidad de Murcia** |\n",
        "|-----------------------------|---------------------------|\n",
        "| ![](https://www.um.es/image/layout_set_logo?img_id=175281&t=1726728636242) | ![](https://www.um.es/o/um-lr-principal-um-home-theme/images/logo-um.png) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KMAdaGOGWv1"
      },
      "source": [
        "# **Práctica 2.  Aprendizaje en entornos complejos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXLaX6uZGWv2"
      },
      "source": [
        "## **1. Preparación del Entorno**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GervG_N0GWv3"
      },
      "source": [
        "### 1.1 Introducción.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZtQiiYzGWv3"
      },
      "source": [
        "Este notebook se enfoca en la implementación y análisis de un agente que utiliza **Aprendizaje por Refuerzo** para resolver distintos entornos que nos ofrece `gymnasium`.  \n",
        "\n",
        "Los entornos que vamos a utlizar son:\n",
        "\n",
        " - **FrozenLake (4x4)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFpQlX2oGWv4"
      },
      "source": [
        "### 1.2. Instalación de Dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "95c1jiWIGWv4"
      },
      "source": [
        "Se instalan las librerías necesarias para trabajar con el entorno de simulación `gymnasium`, lo que permite crear un ambiente controlado donde el agente pueda interactuar y aprender. Este entorno simula una cuadrícula donde el agente debe navegar para alcanzar una meta, mientras se enfrenta a varios desafíos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "REXKqijvGWv5"
      },
      "outputs": [],
      "source": [
        "!  git clone https://github.com/GonzaloMA-17/RL_FJLGM.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd RL_FJLGM"
      ],
      "metadata": {
        "id": "k4wjwRWGGu3N",
        "outputId": "945be1c6-701d-4b37-abfa-595fe9f6aa38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'RL_FJLGM'\n",
            "/content/RL_FJLGM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p_4uevDpGWv6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# ! pip install 'gym[box2d]==0.20.0'\n",
        "# ! pip install gymnasium[box2d]\n",
        "# ! pip install \"gymnasium[toy-text]\n",
        "# ! pip install gymnasium\n",
        "# ! pip install numpy\n",
        "# ! pip install matplotlib\n",
        "# ! pip install tqdm\n",
        "# ! pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lX7xNirGWv7"
      },
      "source": [
        "### 1.3 Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wcORu3ukGWv7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import gc\n",
        "import torch\n",
        "from src_agents import *\n",
        "from src_plotting import *\n",
        "\n",
        "from src_agents.deepQLearning import *\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from src_agents.sarsaSemiGradiente import FrozenLakeWrapper, train_sarsa#, GraphVisualizer\n",
        "from src_plotting import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JDUQIXqGWv7"
      },
      "source": [
        "### 1.4 Importación de los Entornos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qOptRNPGWv8"
      },
      "source": [
        "Se cargan dos entornos diferentes para el agente:\n",
        "- **FrozenLake (4x4)** es un pequeño lago congelado en el que el agente debe navegar para llegar a su destino, evitando caer en el agua. Este entorno es especialmente útil para un aprendizaje inicial, ya que no es resbaladizo, lo que facilita la comprensión de cómo el agente interactúa con el entorno.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbWZNW55GWv8"
      },
      "source": [
        "\n",
        "### 1.5 Funciones para Mostrar los Resultados\n",
        "Se definen varias funciones para graficar y visualizar los resultados del entrenamiento del agente. Estas funciones permiten analizar el desempeño del agente en diferentes aspectos.\n",
        "1.`plot(list_stats)`  \n",
        "   - **Descripción**: Graficará la proporción de recompensas obtenidas en cada episodio del entrenamiento. Este gráfico proporciona una visualización clara del progreso del agente, mostrando cómo mejora su desempeño a medida que avanza en el proceso de aprendizaje.\n",
        "\n",
        "2. `plot_episode_lengths(episode_lengths, window=50)`  \n",
        "   - **Descripción**: Graficará la longitud de los episodios en el entrenamiento, y calculará y mostrará la tendencia de las longitudes utilizando una media móvil.\n",
        "\n",
        "3. `plot_policy_blank(Q, env)`  \n",
        "   - **Descripción**: Dibuja la política aprendida por el agente sobre un entorno vacío, representando las acciones óptimas en cada estado de la **Q-table** del agente con flechas (izquierda, abajo, derecha, arriba).\n",
        "\n",
        "4. `plot_comparison(stats_list, labels, title=\"Comparación de Resultados de Entrenamiento\")`  \n",
        "   - **Descripción**: Compara la evolución de las recompensas obtenidas por diferentes algoritmos o agentes a lo largo del entrenamiento, permitiendo visualizar el rendimiento relativo de cada uno.\n",
        "\n",
        "5. `plot_episode_lengths_comparison(episode_lengths_list, labels, window=50, title=\"Comparación de Longitudes de Episodios\")`  \n",
        "   - **Descripción**: Compara las longitudes de los episodios de diferentes agentes, mostrando las longitudes y las tendencias de cada uno en subgráficas separadas, con sombreado y media móvil.\n",
        "\n",
        "Todas las funciones y clases relacionadas con la visualización de gráficos se encuentran en el archivo `src_agents/plotting.py`. Para utilizarlas, será necesario importar las librerías correspondientes desde esa ruta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Znjnc6QGWv8"
      },
      "source": [
        "## **2. Diseño del Agente**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_-bwY26GWv8"
      },
      "source": [
        "### 2.1 Estructura del Agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtbip1w6GWv8"
      },
      "source": [
        "Este es la estructura que contiene la implementación de distintos agentes, organizados en módulos específicos para facilitar su uso y mantenimiento.  \n",
        "\n",
        "Cada agente hereda de una clase base (`agent.py`) y utiliza diferentes estrategias de aprendizaje, como **Monte Carlo**, **Q-Learning** y **SARSA**. Además, se incluyen módulos auxiliares para definir políticas de exploración y visualizar resultados.\n",
        "\n",
        "```plaintext\n",
        "|-- 📂 src_agents                                    # Carpeta principal que contiene los agentes de Aprendizaje por Refuerzo\n",
        "|   |-- 📄 __init__.py                               # Archivo que convierte el directorio en un paquete de Python\n",
        "|   |-- 📄 agent.py                                  # Clase base para todos los agentes\n",
        "|   |-- 📄 deepQLearning.py                          # Implementación del agente Deep Q-Learning (DQN)\n",
        "|   |-- 📄 monteCarloOnPolicy.py                     # Implementación del agente Monte Carlo On-Policy\n",
        "|   |-- 📄 monteCarloOffPolicy.py                    # Implementación del agente Monte Carlo Off-Policy\n",
        "|   |-- 📄 qLearning.py                              # Implementación del agente Q-Learning\n",
        "|   |-- 📄 sarsa.py                                  # Implementación del agente SARSA tabular\n",
        "|   |-- 📄 sarsaSemiGradiente.py                     # Implementación del agente SARSA Semigradiente\n",
        "|   |-- 📄 politicas.py                              # Definición de políticas de exploración como epsilon-greedy y softmax\n",
        "\n",
        "|-- 📂 src_plotting                                  # Carpeta con herramientas de visualización de resultados\n",
        "|   |-- 📄 __init__.py                               # Archivo que convierte el directorio en un paquete de Python\n",
        "|   |-- 📄 plotting.py                               # Funciones de visualización de datos y gráficos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXLfkgB6GWv9"
      },
      "source": [
        "El diseño del agente consta de dos partes.  \n",
        "\n",
        "- Las políticas (toma de decisiones) que realiza.\n",
        "- El algoritmo con el que aprende."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x0Wqu52GWv9"
      },
      "source": [
        "### 2.2 Políticas del Agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R8GbIKwGWv9"
      },
      "source": [
        "- **Epsilon-soft**: Se define una política donde todas las acciones tienen una probabilidad de ser elegida.\n",
        "   \n",
        "- **Política epsilon-greedy**: basada en la política epsilon-soft. De esta forma el agente tiene una pequeña probabilidad de explorar (tomar una acción aleatoria) y una mayor probabilidad de explotar (tomar la acción que considera mejor). Esto permite equilibrar la exploración y la explotación.  \n",
        "\n",
        "- **Política greedy**: Es la usada una vez que \"ha aprendido\".  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nztilHa-GWv9"
      },
      "source": [
        "### 2.3 Algoritmos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkHbYqxPGWv9"
      },
      "source": [
        "\n",
        "- **Tabulares**  \n",
        "  - Monte Carlo On Policy  \n",
        "  - Monte Carlo Off Policy  \n",
        "  - SARSA\n",
        "  - Q - Learning\n",
        "\n",
        "- **Control con Aproximaciones**\n",
        "  - SARSA semigradiente\n",
        "  - Deep Q - Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jpVkuEhGWv9"
      },
      "source": [
        "## **3. Experimentación - Frozen Lake**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qdmJWoRGWv9"
      },
      "source": [
        "FrozenLake es un entorno de Aprendizaje por Refuerzo incluido en Gymnasium, en el que un agente debe aprender a desplazarse sobre una superficie helada para llegar a un objetivo sin caer en agujeros. Se representa como una cuadrícula donde cada celda puede ser suelo firme, un agujero o la meta.\n",
        "\n",
        "El agente puede moverse en cuatro direcciones: izquierda, derecha, arriba y abajo. En la versión estándar del entorno, el hielo introduce un factor de aleatoriedad en los movimientos, lo que significa que el agente no siempre se desplaza en la dirección elegida. Sin embargo, en este caso **configuraremos el entorno sin deslizamiento**, lo que significa que el agente se moverá exactamente en la dirección que elija sin desviaciones aleatorias. Esto hace que el problema sea más determinista y permite un aprendizaje más directo de las estrategias óptimas.\n",
        "\n",
        "El objetivo del agente es llegar a la casilla de meta desde la posición inicial sin caer en un agujero. Se utiliza un sistema de recompensas en el que el agente recibe una recompensa de 1 cuando alcanza la meta y 0 en cualquier otro caso. Al eliminar el deslizamiento, el agente puede aprender una política óptima de manera más eficiente, ya que sus acciones tienen consecuencias predecibles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b5BO0kGGWv-"
      },
      "source": [
        "### 3.5 Deep Q - Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTA165lJGWv-"
      },
      "source": [
        "# Deep Q-Learning (DQN)\n",
        "\n",
        "Deep Q-Learning (DQN) es una extensión de Q-Learning que utiliza redes neuronales para aproximar la función de valor de estado-acción $ Q(s, a) $, permitiendo manejar espacios de estado continuos o de alta dimensión donde el enfoque tabular no es viable.\n",
        "\n",
        "\n",
        "DQN utiliza una red neuronal que recibe el estado $ s $ como entrada y predice los valores $ Q(s, a) $ para cada acción posible. La actualización de $ Q(s, a) $ sigue la ecuación:\n",
        "\n",
        "$y = r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a')$\n",
        "\n",
        "donde $ Q_{\\text{target}} $ es una copia de la red principal que se actualiza periódicamente para estabilizar el entrenamiento.\n",
        "\n",
        "\n",
        "- **Replay Buffer**: Se almacenan transiciones $ (s, a, r, s') $ para romper la correlación entre experiencias y mejorar la estabilidad del aprendizaje.  \n",
        "- **Target Network**: Se mantiene una red objetivo congelada que se actualiza cada cierto número de pasos para evitar oscilaciones en el entrenamiento.  \n",
        "- **Exploración con $ \\epsilon $-greedy**: Se usa $ \\epsilon $-greedy con un decaimiento progresivo de $ \\epsilon $ para favorecer la exploración al inicio y la explotación en etapas avanzadas.  \n",
        "- **Función de pérdida**: Se minimiza el error cuadrático medio (MSE) entre la predicción de la red y la estimación corregida:\n",
        "\n",
        "$L(\\theta) = \\mathbb{E} \\left[ \\left( y - Q(s, a; \\theta) \\right)^2 \\right]$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4tm_vsUGWv-"
      },
      "source": [
        "#### Mapa 4x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bapmIivbGWv-"
      },
      "source": [
        "Este fragmento de código establece una semilla fija para la generación de números aleatorios en varias bibliotecas, lo que garantiza la **reproducibilidad** en experimentos de Machine Learning o Aprendizaje por Refuerzo.\n",
        "\n",
        "- `seed = 1995`: Se define una semilla fija con el valor `1995`.\n",
        "\n",
        "- `random.seed(seed)`: Establece la semilla para la librería `random` de Python.\n",
        "\n",
        "- `np.random.seed(seed)`: Fija la semilla para `NumPy`, garantizando que las funciones aleatorias de numpy generen los mismos valores en ejecuciones repetidas.\n",
        "\n",
        "- `torch.manual_seed(seed)`: Configura la semilla en PyTorch para que las operaciones aleatorias en tensores de CPU sean reproducibles.\n",
        "\n",
        "- `torch.cuda.manual_seed(seed)`: Si hay una GPU, fija la semilla en CUDA para asegurar la reproducibilidad en cálculos realizados en la GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PcNvJfkCGWv-"
      },
      "outputs": [],
      "source": [
        "seed = 1995\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s0qtTmWGWv-"
      },
      "source": [
        "El siguiente código crea un entorno de **FrozenLake** en Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2Z5AiFVAGWv-"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', is_slippery=False, map_name=\"4x4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDVs_ruJGWv-"
      },
      "source": [
        "Realizamos el entrenamiento de un agente en el entorno **FrozenLake** utilizando el algoritmo **Deep Q-Learning (DQN)**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJh1x5CAGWv_",
        "outputId": "4c43f67d-5782-4536-c1ab-241d6ed73d14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50, Average Reward: 0.00, Epsilon: 0.78\n",
            "Episode 100, Average Reward: 0.02, Epsilon: 0.61\n",
            "Episode 150, Average Reward: 0.26, Epsilon: 0.47\n",
            "Episode 200, Average Reward: 0.62, Epsilon: 0.37\n",
            "Episode 250, Average Reward: 0.72, Epsilon: 0.29\n",
            "Episode 300, Average Reward: 0.68, Epsilon: 0.22\n",
            "Episode 350, Average Reward: 0.76, Epsilon: 0.17\n",
            "Episode 400, Average Reward: 0.82, Epsilon: 0.13\n",
            "Episode 450, Average Reward: 0.84, Epsilon: 0.10\n",
            "Episode 500, Average Reward: 0.88, Epsilon: 0.08\n",
            "Episode 550, Average Reward: 0.92, Epsilon: 0.06\n",
            "Episode 600, Average Reward: 0.90, Epsilon: 0.05\n",
            "Episode 650, Average Reward: 1.00, Epsilon: 0.04\n",
            "Episode 700, Average Reward: 0.98, Epsilon: 0.03\n",
            "Episode 750, Average Reward: 1.00, Epsilon: 0.02\n",
            "Episode 800, Average Reward: 1.00, Epsilon: 0.02\n",
            "Episode 850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 1050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1250, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 1300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 1350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 1500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 1550, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 1600, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 1650, Average Reward: 0.62, Epsilon: 0.01\n",
            "Episode 1700, Average Reward: 0.88, Epsilon: 0.01\n",
            "Episode 1750, Average Reward: 0.72, Epsilon: 0.01\n",
            "Episode 1800, Average Reward: 0.08, Epsilon: 0.01\n",
            "Episode 1850, Average Reward: 0.38, Epsilon: 0.01\n",
            "Episode 1900, Average Reward: 0.54, Epsilon: 0.01\n",
            "Episode 1950, Average Reward: 0.50, Epsilon: 0.01\n",
            "Episode 2000, Average Reward: 0.28, Epsilon: 0.01\n",
            "Episode 2050, Average Reward: 0.74, Epsilon: 0.01\n",
            "Episode 2100, Average Reward: 0.60, Epsilon: 0.01\n",
            "Episode 2150, Average Reward: 0.34, Epsilon: 0.01\n",
            "Episode 2200, Average Reward: 0.26, Epsilon: 0.01\n",
            "Episode 2250, Average Reward: 0.62, Epsilon: 0.01\n",
            "Episode 2300, Average Reward: 0.76, Epsilon: 0.01\n",
            "Episode 2350, Average Reward: 0.88, Epsilon: 0.01\n",
            "Episode 2400, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 2450, Average Reward: 0.62, Epsilon: 0.01\n",
            "Episode 2500, Average Reward: 0.44, Epsilon: 0.01\n",
            "Episode 2550, Average Reward: 0.62, Epsilon: 0.01\n",
            "Episode 2600, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 2650, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 2700, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 2750, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 2800, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 2850, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 2900, Average Reward: 0.70, Epsilon: 0.01\n",
            "Episode 2950, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 3000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 3050, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 3100, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 3150, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 3200, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 3250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 3300, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 3350, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 3400, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 3450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 3500, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 3550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 3600, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 3650, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 3700, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 3750, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 3800, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 3850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 3900, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 3950, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 4000, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 4050, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4100, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 4150, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 4200, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 4250, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 4300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 4350, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 4400, Average Reward: 0.82, Epsilon: 0.01\n",
            "Episode 4450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 4500, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 4550, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4600, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 4650, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4700, Average Reward: 0.88, Epsilon: 0.01\n",
            "Episode 4750, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4800, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4850, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 4900, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 4950, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 5000, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 5050, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 5100, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 5150, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 5200, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 5250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 5300, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 5350, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 5400, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 5450, Average Reward: 0.78, Epsilon: 0.01\n",
            "Episode 5500, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 5550, Average Reward: 0.82, Epsilon: 0.01\n",
            "Episode 5600, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 5650, Average Reward: 0.70, Epsilon: 0.01\n",
            "Episode 5700, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 5750, Average Reward: 0.86, Epsilon: 0.01\n",
            "Episode 5800, Average Reward: 0.86, Epsilon: 0.01\n",
            "Episode 5850, Average Reward: 0.66, Epsilon: 0.01\n",
            "Episode 5900, Average Reward: 0.76, Epsilon: 0.01\n",
            "Episode 5950, Average Reward: 0.78, Epsilon: 0.01\n",
            "Episode 6000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6050, Average Reward: 0.84, Epsilon: 0.01\n",
            "Episode 6100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 6300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6350, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 6400, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 6450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 6500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6550, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 6600, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 6650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 6750, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 6800, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 6850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 6950, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 7000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7050, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7150, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7250, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 7300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7400, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7550, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7650, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7800, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 7850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 7950, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 8000, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 8050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 8500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 8550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8650, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 8700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 8950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9100, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9150, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 9200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9350, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 9400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9550, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9750, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 9850, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9900, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 9950, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10150, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10750, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 10800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10850, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 10900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 10950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11150, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 11200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 11300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11450, Average Reward: 0.92, Epsilon: 0.01\n",
            "Episode 11500, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 11550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11650, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 11700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 11750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 11800, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 11850, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 11900, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 11950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12100, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 12150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 12300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12400, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 12450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12650, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 12700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12850, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 12900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 12950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13650, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 13900, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 13950, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 14000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 14050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 14250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 14550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 14900, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 14950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15050, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 15100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15150, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 15200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15300, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 15350, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 15400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 15550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 15950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16050, Average Reward: 0.90, Epsilon: 0.01\n",
            "Episode 16100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16350, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 16400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16650, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16800, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 16850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 16950, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17000, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17050, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 17100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17400, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17500, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17600, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17750, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 17800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 17950, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18050, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18100, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18200, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 18550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18600, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18650, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 18700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18800, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 18850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 18900, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 18950, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 19000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19350, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19450, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19550, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19600, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19700, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 19750, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19850, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19900, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 19950, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20150, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20250, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20300, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20450, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20500, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20550, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20600, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20650, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20700, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20750, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20800, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 20850, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 20900, Average Reward: 0.80, Epsilon: 0.01\n",
            "Episode 20950, Average Reward: 0.94, Epsilon: 0.01\n",
            "Episode 21000, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 21050, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 21100, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 21150, Average Reward: 0.96, Epsilon: 0.01\n",
            "Episode 21200, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 21250, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 21300, Average Reward: 0.98, Epsilon: 0.01\n",
            "Episode 21350, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 21400, Average Reward: 1.00, Epsilon: 0.01\n",
            "Episode 21450, Average Reward: 1.00, Epsilon: 0.01\n"
          ]
        }
      ],
      "source": [
        "episode_rewards_DQL, episode_lengths_DQL, training_errors_DQL, policy_net, target_net_DQL = train_dqn(env, num_episodes=25000, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw-IWMX4GWv_"
      },
      "source": [
        "**Conclusión**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QrvtFZ8GWv_"
      },
      "outputs": [],
      "source": [
        "plot_all(episode_rewards_DQL, episode_lengths_DQL, training_errors_DQL, rolling_length=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPYTYBmNGWv_"
      },
      "source": [
        "Analizando las gráficas obtenidas durante el entrenamiento del agente con DQN en el entorno FrozenLake 4x4, podemos destacar los siguientes puntos:\n",
        "\n",
        "- **Evolución de las recompensas** (Gráfica izquierda)\n",
        "Al comienzo del entrenamiento, la recompensa media es baja y presenta fluctuaciones considerables, reflejando la fase de exploración inicial del agente.\n",
        "A medida que avanzan los episodios (antes de llegar a los 2,000 aproximadamente), la recompensa aumenta de manera notable hasta estabilizarse cerca de valores próximos a 1. Esto indica que el agente ha aprendido a maximizar su probabilidad de alcanzar la meta.\n",
        "En la etapa final, la recompensa se mantiene estable en valores altos, lo cual sugiere que el agente ha convergido a una política casi óptima, pese a la naturaleza estocástica del entorno.\n",
        "\n",
        "- **Evolución de la longitud de los episodios** (Gráfica derecha)\n",
        "Durante los primeros episodios, se observan picos de duración muy elevados (llegando a superar 80 pasos), señal de que el agente todavía no cuenta con una estrategia clara y está probando múltiples rutas sin éxito.\n",
        "Con el progreso del entrenamiento, la longitud de los episodios desciende drásticamente, llegando a estabilizarse en valores bajos (cercanos a 5–10 pasos), lo que indica que el agente ha encontrado rutas eficientes para resolver el entorno.\n",
        "Hacia la fase final, la longitud se mantiene relativamente constante, reforzando la idea de que el agente aplica una política sólida para llegar a la meta con el menor número de movimientos posibles.\n",
        "\n",
        "**Conclusión general**\n",
        "El entrenamiento con Deep Q-Learning en FrozenLake 4x4 demuestra ser exitoso. El agente pasa de episodios largos con recompensas bajas a episodios más cortos y recompensas cercanas al máximo. La estabilidad observada en ambos gráficos hacia el final del entrenamiento indica que el agente ha aprendido una política efectiva y robusta para navegar en un entorno estocástico, maximizando su recompensa y reduciendo los movimientos innecesarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNNLjJopGWv_"
      },
      "source": [
        "### 3.6 Sarsa SemiGradiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7ZYPYcKGWv_"
      },
      "source": [
        "SARSA semigradiente es una extensión del algoritmo SARSA tradicional, diseñada para manejar entornos con espacios de estado continuos o de alta dimensión. En estos casos, la representación tabular de $ Q(s, a) $ se vuelve inviable, por lo que se emplean funciones de aproximación en lugar de almacenar valores discretos para cada par estado-acción.\n",
        "\n",
        "El método SARSA semigradiente utiliza una función de aproximación de la forma:\n",
        "\n",
        "$$\n",
        "Q(s, a; \\theta) \\approx f(s, a; \\theta)\n",
        "$$\n",
        "\n",
        "donde $ \\theta $ representa un conjunto de parámetros ajustables que determinan la estimación de los valores $ Q(s, a) $. En lugar de actualizar una tabla de valores $ Q $, se actualizan los parámetros $ \\theta $ mediante descenso de gradiente.\n",
        "\n",
        "La actualización de los parámetros $ \\theta $ sigue la regla de aprendizaje:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_{\\theta} Q(s, a; \\theta)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $\\alpha $ es la tasa de aprendizaje.\n",
        "- $ \\delta $ es el error temporal definido como:\n",
        "\n",
        "$$\n",
        "\\delta = R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}; \\theta) - Q(s, a; \\theta)\n",
        "$$\n",
        "\n",
        "- $ \\nabla_{\\theta} Q(s, a; \\theta) $ es el gradiente de la función de aproximación respecto a los parámetros $ \\theta $.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFx8126dGWwA"
      },
      "source": [
        "#### Mapa 4x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cgomsbIGWwF"
      },
      "source": [
        "Comenzamos el código estableciendo la semilla para los generadores de números aleatorios en varias bibliotecas, asegurando que los experimentos sean reproducibles.\n",
        "- **`seed = 1995`**: Se define el valor de la semilla como `1995`.\n",
        "- **`random.seed(seed)`**: Establece la semilla en el módulo `random` de Python.\n",
        "-\n",
        "- **`np.random.seed(seed)`**: Establece la semilla en el generador de números aleatorios de NumPy.\n",
        "-\n",
        "- **`torch.manual_seed(seed)`**: Establece la semilla para el generador de números aleatorios de PyTorch en la CPU.\n",
        "-\n",
        "- **`if torch.cuda.is_available(): torch.cuda.manual_seed(seed)`**: Si se dispone de una GPU (es decir, si CUDA está disponible), esta línea establece la semilla para el generador de números aleatorios de PyTorch en la GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FngIQNb0GWwF"
      },
      "outputs": [],
      "source": [
        "seed = 1995\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ul_hALYGWwG"
      },
      "source": [
        "En este fragmento de código, se está creando un entorno utilizando la clase `FrozenLakeWrapper`. con el parametro ``is_slippery = False``, para crear un entorno que no sea resbaladizo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JsSvKHGGWwG"
      },
      "outputs": [],
      "source": [
        "# Usamos FrozenLakeWrapper para crear el entorno\n",
        "env_wrapper = FrozenLakeWrapper(is_slippery=False, map_name=\"4x4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FPkOqzcGWwG"
      },
      "source": [
        "Se entrena el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHZ7RSSWGWwG"
      },
      "outputs": [],
      "source": [
        "# Entrenar usando SARSA semigradiente (en vez de DQN)\n",
        "episode_rewards_SSG, episode_lengths_SSG, training_errors_SSG, agent = train_sarsa(env_wrapper, num_episodes=25000, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwRIjKR7GWwH"
      },
      "source": [
        "**Conclusión**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3copHcJBGWwH"
      },
      "outputs": [],
      "source": [
        "plot_all(episode_rewards_SSG, episode_lengths_SSG, training_errors_SSG, rolling_length=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUAgP09rGWwH"
      },
      "source": [
        "Analizando las gráficas obtenidas durante el entrenamiento del agente con **Sarsa semigradiente** en el entorno **FrozenLake**, podemos destacar los siguientes puntos:\n",
        "\n",
        "\n",
        "**Evolución de las recompensas** (Gráfica izquierda)\n",
        "    - Al inicio del entrenamiento, las recompensas son bajas y muestran una alta variabilidad, lo cual indica que el agente se encuentra en fase de exploración y aún no ha desarrollado una estrategia clara.  \n",
        "    - A medida que avanza el entrenamiento (alrededor de los primeros miles de episodios), la recompensa media asciende rápidamente, situándose cerca de valores de 0.8–0.9. Esto sugiere que el agente aprende a tomar decisiones que le permiten alcanzar la meta con mayor frecuencia.  \n",
        "    - En la fase final, la recompensa se mantiene relativamente estable cerca de valores altos, lo que indica que el agente ha convergido a una política casi óptima.\n",
        "\n",
        "\n",
        "**Evolución de la longitud de los episodios** (Gráfica derecha)\n",
        "    - Durante los primeros episodios, la longitud de los mismos es relativamente alta y presenta fluctuaciones considerables, reflejando la exploración intensa del agente y la ausencia de una ruta clara hacia la meta.  \n",
        "    - Conforme el agente adquiere experiencia, se observa una tendencia a la baja en la duración de los episodios. Esto indica que el agente aprende a llegar a la meta en menos pasos o, en caso de fallar, lo hace antes, reduciendo el tiempo total del episodio.  \n",
        "    - En la etapa final del entrenamiento, la longitud de los episodios se estabiliza en valores más bajos, lo que demuestra que el agente ha aprendido la ruta más eficiente para resolver el entorno con el mínimo número de movimientos posibles.\n",
        "\n",
        "**Conclusión general**\n",
        "    El entrenamiento con **Sarsa semigradiente** en **FrozenLake** muestra resultados exitosos. El agente pasa de una fase de exploración errática, con bajas recompensas y episodios largos, a una estrategia consistente que maximiza la recompensa y minimiza la duración de los episodios. Aunque el entorno estocástico impide alcanzar un éxito absoluto, el rendimiento final del agente es alto y refleja la convergencia hacia una política efectiva y estable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvk52b0YGWwH"
      },
      "source": [
        "## **4. Conclusión**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LrqqtwpGWwI"
      },
      "outputs": [],
      "source": [
        "plot_comparative_results(episode_rewards_SSG, episode_lengths_SSG,\n",
        "                         episode_rewards_DQL, episode_lengths_DQL,\n",
        "                         label1=\"SARSA Semigradiente\", label2=\"DQN\",\n",
        "                         rolling_length=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7bPtnNHGWwI"
      },
      "source": [
        "Analizando las gráficas comparativas de Sarsa Semigradiente (línea azul) y DQN (línea roja punteada) en el entorno FrozenLake 4x4, se pueden destacar los siguientes puntos:\n",
        "\n",
        "**Comparación de Recompensas** (Gráfica izquierda)\n",
        "- Fase inicial:\n",
        "Tanto Sarsa Semigradiente como DQN comienzan con recompensas bajas e inestables, reflejando la etapa de exploración y la falta de una estrategia definida.  \n",
        "\n",
        "- Progreso:\n",
        "Sarsa Semigradiente muestra un ascenso paulatino hasta situarse cerca de valores de recompensa cercanos a 1, manteniéndose relativamente estable en ese rango.\n",
        "DQN experimenta un incremento más rápido en las recompensas, pero presenta picos de inestabilidad en algunos tramos, donde la recompensa cae bruscamente antes de recuperarse.\n",
        "\n",
        "Ambos métodos alcanzan recompensas promedio altas, lo que indica que aprenden políticas eficaces para superar el entorno. Sin embargo, DQN exhibe mayor variabilidad.\n",
        "\n",
        "**Comparación de Longitudes de Episodios** (Gráfica derecha)\n",
        "\n",
        "- Fase inicial:\n",
        "Al inicio, la duración de los episodios para DQN es muy elevada, mientras que Sarsa Semigradiente se mantiene en rangos algo más moderados, aunque también inestables.  \n",
        "\n",
        "- Progreso:\n",
        "Con el paso de los episodios, ambas estrategias reducen significativamente la longitud media de los episodios. Esto sugiere que, al aprender la dinámica del entorno, el agente encuentra rutas más eficientes para llegar a la meta o, en caso de fallar, lo hace de forma más rápida.\n",
        "\n",
        "Sarsa Semigradiente logra estabilizarse en valores relativamente bajos, mostrando una curva más homogénea.\n",
        "DQN reduce también la duración de los episodios a valores bajos, pero sufre picos ocasionales que indican cierta inestabilidad en la política aprendida.\n",
        "\n",
        "**Conclusión general**\n",
        "Tanto Sarsa Semigradiente como DQN consiguen resolver de manera efectiva el entorno FrozenLake 4x4, alcanzando recompensas altas y reduciendo drásticamente la longitud de los episodios. No obstante, se aprecian diferencias en la estabilidad del aprendizaje:\n",
        "\n",
        "Basandonos únicamente en la estabilidad y la consistencia de los resultados, Sarsa Semigradiente parece ser la mejor opción en este experimento con FrozenLake 4x4. Aunque DQN puede alcanzar recompensas similares o incluso aprender un poco más rápido en ciertos tramos, presenta más fluctuaciones tanto en la recompensa como en la longitud de los episodios, lo que indica una política menos estable.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}